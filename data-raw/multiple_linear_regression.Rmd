---
title: "Multiple Linear Regression in R"
author: "Bruno Dos Santos"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Introduction

In this Notebook, we present the methodology that we used to perform a multiple linear regression. We use data from [Kaggle](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction), named *"House Sales in King County, USA"*, to predict house price using a regression model. The data set contains house sales prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015, with 21,613 registers and 21 variables regarding the houses sold.

## R notebook

Load R packages used in this notebook:

```{r}
library("here")
library("MASS")
library("lmtest")
library("psych")
library("regclass")
```

Reading the housing dataset and creating a data frame:
```{r }
df <- as.data.frame(read.csv(here("data-raw","kc_house_data.csv")))
```

Visualization of the first five lines of the data frame:
```{r}
head(df,5)
```

The variable `ID` will not be used to perform the regression model, so we will remove if from the data set. Also, the variable date, regarding the date of house sell, does not bring much information because the time range is a few higher than 1 year, so we'll exclude it too. The next chuck select only the variables that we will use for modelling:

```{r}
df <- df[,3:21]
```

Visualizing some statistics about the data:

```{r}
summary(df)
```

Now, let's exclude possible missing values. First, we'll count the row number of our data frame:

```{r}
nrow(df)
```

Now, we'll exclude the missing values (NA values), and then, count the row number again:

```{r}
df <- na.omit(df)
nrow(df)
```

The number didn't change, which means that there were no missing values in our data frame. Visualizing the type of the variables:

```{r, cache=TRUE}
str(df)
```

All the variables in our data frame are numeric or integer, which means that we don't need to apply any process to convert the data type in order to run the regression model.

## Regression models

Let's perform the first regression model. In this case, we will use all variables of the data set to predict the price of the house:

```{r}
model_lr1 <- lm(df$price ~ . , data = df)
```

Let's visualize some information about our first model:

```{r}
summary(model_lr1)
```

We can see that some coefficients of the first model are not significant, with Pr(\>\|t\|) \> Signif. codes. So, we'll apply a search mode for feature selection. The idea is minimizing the Akaike Information Criteria (AIC) and simplify the model:

```{r}
model_lr2 <- stepAIC(model_lr1, direction = "both", trace = 1)
```

Let's visualize some information about our second model:

```{r}
summary(model_lr2)
```

According to the summary above, all coefficients are significant. We can check the value of the R-squared, a statistical measure that determines the proportion of variance in the dependent variable that can be explained by the independent variables.

## Diagnosing the regression model

Let's use some diagnostic plots about our regression model (model 2).

-   The first graph is a residual versus fitted values graph, which assesses the linearity of the model. If linearity exists, the red line should look like a horizontal line.

-   The second graph, the Q-Q graph, assesses whether the residuals have a normal distribution. In this case, if the residual distribution is normal, most of the residuals should be above the line.

-   The third graph, Scale-Location, assesses homoscedasticity, where the points must be dispersed in a rectangular (non-triangular) way.

-   The fourth graph assesses the existence of influential points and outliers.

```{r}
plot(model_lr2)
```

According to the graphs above, we can see that some points (3915,7253,9255,15871, and others) behave abnormally compared to the others, and they could affect the final regression model. For this reason, we will remove these points from the data set and run the regressions again. These diagnostic steps can be run more than once, with the aim of finding the best possible model.

```{r}
#df <- df[-c(3915,7253,9255,15871,4412,1316,1165,1449,8639),]
df <- df[-c(18483,2627,6692,8093,12371,4150),]
```

**ðŸ“ *After remove the abnormal points, we need to perform the regression model section again.***

**After perform the Regression models section again,** we can see that the R-squared increased, showing an improved in the new model.

However, analyzing the model using graphs can be difficult for some people. In order to be more objective, we can use some tests to evaluate our model. One such test is the Shapiro test, used to assess the normality of the residuals. We consider a distribution to be normal when the p-value is greater than 0.05 (the Shapiro test requires a sample size between 3 and 5000). In our case, the data set has more than 500 rows, so this test cannot be applied.

```{r}
#shapiro.test(model_lr2$residuals)
```

We can use the summary of standard residuals to evaluate the existence of outliers. The minimum and maximum values must have similar absolute values, the mean must be close to zero and the first and third quartiles must have similar absolute values too:

```{r}
summary(rstandard(model_lr2))
```

The Breusch-Pagan test evaluates the homoscedasticity of the residuals. To prove the homoscedasticity, the p-value must be higher than 0.05:

```{r}
bptest(model_lr2)
```

The VIF test evaluates the multicolinarity between the independent variables. In this case, all variables must have a VIF value less than 10:

```{r}
VIF(model_lr2)
```

After apllying the test, we can see that our residuals don't present normality and homoscedasticity. In future applications, we can apply some process to correct this. For the non-normality residuals, we can make transformations to dependent variable and keep removing outliers. For the heteroscedasticity, we can implement other types of regressions or data transformations. Also, the model would be beneficied if the data be splitted into training and test data for modelling.

Finally, we will compare the predicted values with the true values using a scatter plot. In a good model, the predicted values must be close to the blue line:

```{r}
df$predicted_values <- model_lr2$fitted.values

p <- plot(df$predicted_values, df$price, col='red', cex = 0.5, pch = 19)
p <- p + abline(lm(df$price~df$predicted_values), lwd=2,col='blue')
p
```

Finally, we will save our data frame in the data folder:

```{r}
kc_house_data_processed <- df

usethis::use_data(kc_house_data_processed, overwrite = TRUE)
```
